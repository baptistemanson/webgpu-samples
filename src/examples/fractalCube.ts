import { mat4, vec3 } from "gl-matrix";
import {
  cubeVertexArray,
  cubeVertexSize,
  cubeColorOffset,
  cubeUVOffset,
  cubePositionOffset,
} from "../cube";
import glslangModule from "../glslang";
import { updateBufferData } from "../helpers";

/**
 * This demo renders the cube into a texture that is used for rendering on screen but also for the next render, as a texture.
 * There is only one pass.
 * In WebGL, I think we would need two draw calls to get the same effect. The first one would render to a texture, the second one would render this texture in the framebuffer.
 */
export const title = "Fractal Cube";
export const description =
  "This example uses the previous frame's rendering result \
              as the source texture for the next frame.";

// in the text  layout(set = 0, binding = 0), "binding" is the important number for gluing things together in WebGPU.
export async function init(canvas: HTMLCanvasElement) {
  const vertexShaderGLSL = `#version 450
  layout(set = 0, binding = 0) uniform Uniforms {
    mat4 modelViewProjectionMatrix;
  } uniforms;

  layout(location = 0) in vec4 position;
  layout(location = 1) in vec4 color;
  layout(location = 2) in vec2 uv;

  layout(location = 0) out vec4 fragColor;
  layout(location = 1) out vec2 fragUV;

  void main() {
    gl_Position = uniforms.modelViewProjectionMatrix * position;
    fragColor = color;
    fragUV = uv;
  }`;
  // surprising api with the sampler being a uniform separate from the texture. I wonder why?
  const fragmentShaderGLSL = `#version 450
  layout(set = 0, binding = 1) uniform sampler mySampler;
  layout(set = 0, binding = 2) uniform texture2D myTexture;

  layout(location = 0) in vec4 fragColor;
  layout(location = 1) in vec2 fragUV;
  
  layout(location = 0) out vec4 outColor;

  void main() {
    vec4 texColor = texture(sampler2D(myTexture, mySampler), fragUV * 0.8 + 0.1);

    // 1.0 if we're sampling the background
    float f = float(length(texColor.rgb - vec3(0.5, 0.5, 0.5)) < 0.01);

    outColor = mix(texColor, fragColor, f);
  }`;

  const adapter = await navigator.gpu.requestAdapter();
  const device = await adapter.requestDevice();
  const glslang = await glslangModule();

  const aspect = Math.abs(canvas.width / canvas.height); // why would the aspect ratio go negative?
  let projectionMatrix = mat4.create();
  mat4.perspective(projectionMatrix, (2 * Math.PI) / 5, aspect, 1, 100.0);

  const context = canvas.getContext("gpupresent");

  // @ts-ignore:
  const swapChain = context.configureSwapChain({
    device,
    format: "bgra8unorm", // [0,1] 32 bit w transparency
    usage: GPUTextureUsage.OUTPUT_ATTACHMENT | GPUTextureUsage.COPY_SRC,
  });

  const [verticesBuffer, vertexMapping] = device.createBufferMapped({
    // cpu accessible buffer
    size: cubeVertexArray.byteLength,
    usage: GPUBufferUsage.VERTEX, // vertex buffer
  });
  new Float32Array(vertexMapping).set(cubeVertexArray);
  verticesBuffer.unmap(); // hand off to the gpu

  // reflects the shader code. Could be generated by the compiler.
  const bindGroupLayout = device.createBindGroupLayout({
    entries: [
      {
        // Transform
        binding: 0,
        visibility: GPUShaderStage.VERTEX,
        type: "uniform-buffer",
      },
      {
        // Sampler
        binding: 1,
        visibility: GPUShaderStage.FRAGMENT,
        type: "sampler",
      },
      {
        // Texture view
        binding: 2,
        visibility: GPUShaderStage.FRAGMENT,
        type: "sampled-texture",
      },
    ],
  });

  // the pipeline layout is a collection of bind groups layouts.
  const pipelineLayout = device.createPipelineLayout({
    bindGroupLayouts: [bindGroupLayout],
  });
  const pipeline = device.createRenderPipeline({
    layout: pipelineLayout,

    vertexStage: {
      module: device.createShaderModule({
        code: glslang.compileGLSL(vertexShaderGLSL, "vertex"),

        // @ts-ignore
        source: vertexShaderGLSL,
        transform: (source) => glslang.compileGLSL(source, "vertex"),
      }),
      entryPoint: "main",
    },
    fragmentStage: {
      module: device.createShaderModule({
        code: glslang.compileGLSL(fragmentShaderGLSL, "fragment"),

        // @ts-ignore
        source: fragmentShaderGLSL,
        transform: (source) => glslang.compileGLSL(source, "fragment"),
      }),
      entryPoint: "main",
    },

    primitiveTopology: "triangle-list",
    depthStencilState: {
      depthWriteEnabled: true,
      depthCompare: "less", // explicit
      format: "depth24plus-stencil8",
    },
    vertexState: {
      vertexBuffers: [
        // only one vertex buffer, containing all attributes interlaced.
        {
          arrayStride: cubeVertexSize,
          attributes: [
            {
              // position
              shaderLocation: 0,
              offset: cubePositionOffset,
              format: "float4",
            },
            {
              // color
              shaderLocation: 1,
              offset: cubeColorOffset,
              format: "float4",
            },
            {
              // uv
              shaderLocation: 2,
              offset: cubeUVOffset,
              format: "float2",
            },
          ],
        },
      ],
    },

    rasterizationState: {
      cullMode: "back",
    },

    colorStates: [
      {
        format: "bgra8unorm",
      },
    ],
  });

  const depthTexture = device.createTexture({
    // depth texture needs to be explicitely created.
    size: { width: canvas.width, height: canvas.height, depth: 1 }, // 2d
    format: "depth24plus-stencil8",
    usage: GPUTextureUsage.OUTPUT_ATTACHMENT,
  });

  const renderPassDescriptor: GPURenderPassDescriptor = {
    colorAttachments: [
      {
        attachment: undefined, // Attachment is set later
        loadValue: { r: 0.5, g: 0.5, b: 0.5, a: 1.0 },
      },
    ],
    depthStencilAttachment: {
      attachment: depthTexture.createView(),

      depthLoadValue: 1.0,
      depthStoreOp: "store",
      stencilLoadValue: 0,
      stencilStoreOp: "store",
    },
  };

  const uniformBufferSize = 4 * 16; // 4x4 matrix
  const uniformBuffer = device.createBuffer({
    size: uniformBufferSize,
    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
  });

  const cubeTexture = device.createTexture({
    size: { width: canvas.width, height: canvas.height, depth: 1 },
    format: "bgra8unorm",
    usage: GPUTextureUsage.COPY_DST | GPUTextureUsage.SAMPLED,
  });

  const sampler = device.createSampler({
    // cool, samplers are separate from the texture.
    magFilter: "linear",
    minFilter: "linear",
  });

  const uniformBindGroup = device.createBindGroup({
    layout: bindGroupLayout,
    entries: [
      {
        binding: 0,
        resource: {
          buffer: uniformBuffer,
        },
      },
      {
        binding: 1,
        resource: sampler,
      },
      {
        binding: 2,
        resource: cubeTexture.createView(),
      },
    ],
  });

  function getTransformationMatrix() {
    let viewMatrix = mat4.create();
    mat4.translate(viewMatrix, viewMatrix, vec3.fromValues(0, 0, -4));
    let now = Date.now() / 10000;
    mat4.rotate(
      viewMatrix,
      viewMatrix,
      1,
      vec3.fromValues(Math.sin(now), Math.cos(now), 0)
    );

    let modelViewProjectionMatrix = mat4.create();
    mat4.multiply(modelViewProjectionMatrix, projectionMatrix, viewMatrix);

    return modelViewProjectionMatrix as Float32Array;
  }

  return function frame() {
    const swapChainTexture = swapChain.getCurrentTexture();
    renderPassDescriptor.colorAttachments[0].attachment = swapChainTexture.createView();

    const commandEncoder = device.createCommandEncoder();
    const { uploadBuffer } = updateBufferData(
      // custom made function that updates a buffer GPU side, via a transfer of a delta and applying it after via a copy.
      device,
      uniformBuffer,
      0,
      getTransformationMatrix(),
      commandEncoder
    );

    // 1 render pass. The framebuffer is then copied in a texture for the next pass. After a few frames, we should have enough depths.
    // it's a nice usecase of what can be possible with webgpu. In WebGL, I would have to use CopyTexImage2D.
    const passEncoder = commandEncoder.beginRenderPass(renderPassDescriptor);
    passEncoder.setPipeline(pipeline);
    passEncoder.setBindGroup(0, uniformBindGroup); // the 0 corresponds to the layout index in the pipeline layout
    passEncoder.setVertexBuffer(0, verticesBuffer);
    passEncoder.draw(36, 1, 0, 0);
    passEncoder.endPass();

    commandEncoder.copyTextureToTexture(
      {
        texture: swapChainTexture,
      },
      {
        texture: cubeTexture,
      },
      {
        width: canvas.width,
        height: canvas.height,
        depth: 1,
      }
    );

    device.defaultQueue.submit([commandEncoder.finish()]);
    uploadBuffer.destroy();
  };
}
